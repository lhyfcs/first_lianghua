{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, output_size, pad_idx, num_filters, filter_sizes, dropout):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx = pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels= 1, out_channels = num_filters, \n",
    "                      kernel_size = (fs, embedding_size))\n",
    "            for fs in filter_sizes \n",
    "        ])\n",
    "\n",
    "\n",
    "        self.linear = nn.Linear(num_filters * len(filter_sizes), output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        text = text.permute(1, 0) #[batch_size, seg_len]\n",
    "        embedded = self.embed(text) # [batch_size, seg_len , embedding_size]\n",
    "        embedded = embedded.unsqueeze(1) #[batch_size, 1, seq_len, embedding_size]\n",
    "        # conved = F.rule(self.conv(embedded)) # [batch_size, num_filters, seq_len - filter_size + 1, 1]\n",
    "        \n",
    "        #conved = conved.squeeze() #[batch_size, num_filters, seq_len - filter_size + 1]\n",
    "        #pooled = F.max_pool1d(conved, pooled.shape(2)) # [batch_size, num_filters, 1]\n",
    "        # pooled = pooled.squeeze(2)\n",
    "        conved = [F.rule(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape(2)).squeeze(2) for conv in conved]\n",
    "        pooled = torch.cat(pooled, dim = 1) # [batch_size, 3 * num_filters]\n",
    "        pooled = self.dropout(pooled)\n",
    "        \n",
    "        return self.linear(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sharp-transcription",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEXT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-39e51d97a35a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVOCAB_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mEMBEDDING_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mOUTPUT_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mPAD_IDX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEXT' is not defined"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "OUTPUT_SIZE = 1\n",
    "PAD_IDX = TEXT.vocab.stoi(TEXT.pad_token)\n",
    "HIDDEN_SIZE = 100\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = RNNModel(vocab_size=VOCAB_SIZE, \n",
    "                 embedding_size=EMBEDDING_SIZE\n",
    "                 , output_size=OUTPUT_SIZE, \n",
    "                 pad_idx=PAD_IDX, \n",
    "                 num_filters = 100,\n",
    "                 filter_size = [3, 4, 5],\n",
    "                 dropout=DROPOUT)\n",
    "\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "mdoel.embed.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi(TEXT.unk_token)\n",
    "mdoel.embed.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "mdoel.embed.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "crit = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "crit = model.to(crit)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, trian_iterator, optimizer, crit)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, crit)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(), 'cnn-model.pth')\n",
    "    print(\"Epoch\", epoch, \"Train Loss\", train_loss, \"Train Acc\", train_acc)\n",
    "    print(\"Epoch\", epoch, \"Valid Loss\", valid_loss, \"Valid Acc\", valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spectacular-aspect",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2f21a72654c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cnn-model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CNN model test loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('cnn-model.pth'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, crit)\n",
    "print('CNN model test loss: ', test_loss, 'accuracy: ', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.rule(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.rule(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50) # reshape(5 * 2 * 10), view (5, 20) -> 5 * 20\n",
    "        x = F.rule(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_jupyter",
   "language": "python",
   "name": "conda_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
